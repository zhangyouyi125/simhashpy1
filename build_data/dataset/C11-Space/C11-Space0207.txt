宇航学报
JOURNAL OF ASTRONAUTICS
1999年　第1期　No.1　1999



PVM环境下提高并行计算加速比的
数值实验研究*
李　桦　王承尧　王正华
　　摘　要　通过采用区域分割技术和拼接网络的方法,在PVM并行环境下,对三维高超音速绕流流场进行了多机并行计算,得到了较高的加速比。通过数值实验研究,讨论了负载平衡、不同的网格规模对加速比的影响。
　　主题词　TVD隐式格式　三维高超音速流场　多机并行计算　加速比
NUMERICAL RESEARCH OF RAISING SPEEDUP 
OF PARALLEL CALCULATION ON PVM
Li Hua Wang Chengyao Wang Zhenghua
(Department of Aerospace Technology,NUDT.Changsha.410073)
　　Abstract　In this paper,used the domain decomposition technique (DDT) and patched grid,Parallel numerical calculation with TVD implicit finite volume algorithm is carried for 3D hypersonic viscous flowfield on PVM.The high speedup is obtained,and the influnce of blancing load,different grid scale to the speedup is discussed.
　　Key words　TVD implicit scheme 3D hypersonic flowfield Parallel calculation Speedup
1　引言
　　在计算流体力学(CFD)中,由于超级并行计算机能够用来求解大规模的数值计算问题,满足工程设计部门的需要,因此,多机并行计算成为CDF中一个重要的发展方向。近年来,国内外对将多机并行计算与CFD相结合的研究十分重视,在这方面开展了许多有益的工作,取得了很大进展［1,2,3］。
　　本文根据并行虚拟计算机(Parallel Virtual Machine,简称PVM)并行环境下多机系统的特点和要求,利用区域分割技术和拼接网络(Patched Grid)对TVD隐式有限体积法的计算程序进行了重构,在PVM二处理机和四处理机系统上实现了三维钝头双锥体绕流流场的多机并行计算,得到了较高的加速比;在数值实验研究中,讨论了负载平衡、不同的网络规模对并行计算加速比的影响。
2　并行策略
　　为了在PVM多机系统上实现TVD稳式有限体积法的多机并行计算,我们根据PVM环境的特点,采用了下列措施:
　　(1)区域分割技术
　　区域分割技术是多机并行计算中常用的并行策略之一。在多机并行计算中,流场分割的数目一般与处理机台数相对应,它通过将要求解的流场划分为若干个子区,每个处理机分别计算一个或几个子区,来完成整个流场的计算工作。
　　为了进一步说明,以用一个四处理机系统求解二维流场为例。通过区域分割技术,我们将流场划分为四个子区,如图1所示。


图1　二维流场分区示意图
　　二维的NS方程经过离散后可以写为［3］:

上式左端为隐式部分,右端Ri,j是余量,用n层的值求解,为显式部分。
　　对于整个求解区域,合成的代数方程组可以写成:
［A］{δQ}={R}
　　对于二维问题,［A］是由4×4块矩阵组成,即:

式中,在对角线上的块矩阵{Aii}是子区的求解矩阵,矩阵{Aij}表示在i子区的求解过程中,在区域边界上与j子区的关联矩阵。
　　显然,在子区的内点求解过程中,{Aij}均为零矩阵;另外,如果i子区与j子区之间没有公共区域边界,则{Aij}为零矩阵。例如,参照图1,在子区1的求解中,由于该区与第3子区没有公共边界区域边界,则{A13}为零矩阵。
　　在单个处理机上,对于每个子区的求解,采用LU-SSOR迭代方法,即:
L1N-1U1δQ(i)=R(i)-{Aij}δQij
其中,i、j表示子区序号,且i,j=1,2,3,4,且δQij为在子区i的区域边界的求解中用到j的区域边界上的变量增量,Li、Ui的表达式详见文献［5］。
　　(2)拼接网络
　　在多机并行计算中,开销最大的是处理机之间的通讯开销。对于PVM环境来说,其网络性能不高,传输数据的速度较慢,因此,怎样减少处理机之间的数据通讯量,则是在提高多机并行计算加速比时重点考虑的问题。
　　采用区域分割技术进行多机并行计算,在子区中采用什么样的网格体系将直接影响到区域之间,也就是各处理机之间的数据通讯量的大小。
　　在结构网格中,分区的网格体系有两种:拼接网络(Patched Grid)和重叠网格(Overlap Grid)。在重叠网格中,对于n维问题,边界上插值是n维的,而拼接网格的边界上插值为(n-1)维;显然,拼接网格在区域边界上的数据传送量要小于重叠网格。因此,拼接网格适合于本文所构造的多机并行算法［4］。
3　数值实验及结果
　　本文在PVM多机系统上以高超音速钝头双锥体绕流流场的分区多机并行计算为算例,讨论了负载平衡和网格规模对并行计算加速比的影响。在并行计算中,对应于处理机台数,对流场进行了分区,流场分区示意图如果图2所示。
　　计算参数为:
M∞=9.9,　Re∞=2.2×105,　T∞=49.8K,攻角α=4°


(a)流场分二区示意图　　(b)流场分四区示意图
图2　流场分区示意图
3.1　负载平衡对算法加速比的影响
　　在本文研究的多区多机并行计算中,当每个子区进行完一次迭代扫描时,在区域边界上要进行数据交换,也就是说,在这个算法中存在一个同步点。在这种情况下,如果各个子区的计算规模,即负载不同的话,那么规模小的子区在计算完成后需要等待规模大的子区。这样就影响了并行效率。因此,在该多区多机并行算法中,平衡负载是提高并行效率最直接有效的手段。
　　本文采用的平衡负载的方法是静态平衡负载。这种方法对问题的分解和任务的分配只进行一次。它在任务之前进行负载划分。任务的大小及分配给某台处理机的任务数据每台处理机的计算能力确定。
　　整个流场的计算网格为57×41×21,在分区多机并行计算中,各个子区的网格划分如下:
　　二处理机并行计算网络:
　　　　负载平衡前:Ⅰ区的计算网格:37×41×21　Ⅱ区的计算网格:22×41×21
　　　　负载平衡后:Ⅰ区的计算网格:30×41×21　Ⅱ区的计算网格:29×41×21
　　四处理机并行计算网格:
　　　　负载平衡前:Ⅰ区的计算网格:37×21×21
　　　　　　　　　Ⅱ区的计算网格:37×22×21
　　　　　　　　　Ⅲ区的计算网格:22×22×21
　　　　　　　　　Ⅳ区的计算网格:22×21×21
　　　　负载平衡后:Ⅰ区的计算网格:30×21×21
　　　　　　　　　Ⅱ区的计算网格:30×22×21
　　　　　　　　　Ⅲ区的计算网格:29×22×21
　　　　　　　　　Ⅳ区的计算网格:29×21×21　
　　在多机并行计算中,通过平衡负载,有效地提高了整个算法的加速比,其结果如表1所示。
　　表1中加速比Sp的计算公式为:

表1　多机并行计算数值实验结果
加速比二处理机四处理机
负载平衡前1.613.06
负载平衡后1.843.44
　　从表1中可以看到,在我们研究的并行算法中,负载平衡是提高算法的加速比的有效手段。
3.2　网格规模对算法加速比的影响
　　本文在进行计算的数值实验中,观察了不同的网格规模对算法加速比的影响,具体结果如表2所示。
　　从表2看到,在小规模的PVM并行环境中,当处理机台数不变时,所计算的问题规模越大,并行算法的加速比越高。
表2　不同网格规模对加速比的影响
加速比单处理机二处理机四处理机
网格规模:41×31×131.01.773.21
网格规模:57×41×211.01.843.44
网格规模:71×51×311.01.883.59
4　结论
　　(1)本文发展的TVD隐式有限体积法的多机并行算法在SGI工作站为结点,以太网连接的PVM并行环境中对钝头双锥体高速绕流流场进行了多机并行计算数值实验,通过负载平衡的方法较好地提高了算法的加速比,在二处理机上加速比为1.84,在四处理机上加速比为3.44。
　　(2)在数值实验中,讨论了不同的网格规模对算法的并行效率的影响。发现:在小规模的PVM并行环境中,当处理机台数不变时,所计算的问题规模越大,算法的加速比越高。
*国家自然科学基金资助(19482002
作者单位：李桦　王承尧　王正华　(国防科技大学航天技术系。长沙。410073)
参考文献
　[1]Gustason J L,Montry G R and Benner R E.Development of parallel methods for a 1024 processer hypercube.SIAM Journal on scientific and statistical computing,1988,3(4)
　[2]Olssen P,and Johnson S.Adataparallel implemstation of an explicit method for the threedimension comprseeible NS equation.Parallel Computing,1990
　[3]王正华,王承尧.显格式宏任务的多机并行计算.空气动力学报,1995,(2)
　[4]Li,H and Wang C Y.Implicit TVD schemes for the solution of three dimension hypersonic jet flowfield.AIAA paper,96-03165
　[5]李桦.三维超音速/高超音速复杂流场分区多机并行计算与实验验证.国防科技大学博士论文,1996
收稿日期:1996年9月6日,修回日期:1998年1月8日
　　)
